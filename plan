
#PPO in MetaDrive Hard o

train.py \
  --env-id MetaDrive-Tut-Hard-v0 \
  --algo PPO \
  --log-dir MetaDriveHard \
  --num-envs 10 \
  --asynchronous \
  --max-steps 10000000


#TD3 in MetaDrive Easy o
td3_trainer.py --env-id MetaDrive-Tut-Easy-v0
 --log-dir data-MetaDrive-Tut-Easy-v0
 


#TD3 in MetaDrive Hard
td3_trainer.py --env-id MetaDrive-Tut-Hard-v0
 --log-dir data-MetaDrive-Tut-Hard-v0


MetaDrive-Tut-[1,5,10,20,50,100]Env-v0


PPO 1 on...

train.py \
  --env-id MetaDrive-Tut-1Env-v0 \
  --algo PPO \
  --log-dir MetaDrive-Tut-1Env-v0 \
  --num-envs 10 \
  --asynchronous \
  --max-steps 10000000



PPO 5

train.py \
  --env-id MetaDrive-Tut-5Env-v0 \
  --algo PPO \
  --log-dir MetaDrive-Tut-5Env-v0 \
  --num-envs 10 \
  --asynchronous \
  --max-steps 10000000


PPO 10

train.py \
  --env-id MetaDrive-Tut-10Env-v0 \
  --algo PPO \
  --log-dir MetaDrive-Tut-10Env-v0 \
  --num-envs 10 \
  --asynchronous \
  --max-steps 10000000


PPO 20

train.py \
  --env-id MetaDrive-Tut-20Env-v0 \
  --algo PPO \
  --log-dir MetaDrive-Tut-20Env-v0 \
  --num-envs 10 \
  --asynchronous \
  --max-steps 10000000



TD3 1
td3_trainer.py --env-id MetaDrive-Tut-1Env-v0
 --log-dir data-MetaDrive-Tut-1Env-v0


TD3 5
td3_trainer.py --env-id MetaDrive-Tut-5Env-v0
 --log-dir data-MetaDrive-Tut-5Env-v0


TD3 10
td3_trainer.py --env-id MetaDrive-Tut-10Env-v0
 --log-dir data-MetaDrive-Tut-10Env-v0

TD3 20
td3_trainer.py --env-id MetaDrive-Tut-20Env-v0
 --log-dir data-MetaDrive-Tut-20Env-v0
